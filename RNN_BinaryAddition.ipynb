{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This program runs on version 2.0.6\n",
      "keras version 2.0.6\n"
     ]
    }
   ],
   "source": [
    "## This sample program performs binary addition using RNN in Keras\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import keras\n",
    "\n",
    "\n",
    "#change the working directory \n",
    "os.chdir('/notebooks/torch')\n",
    "\n",
    "print(\"This program runs on version 2.0.6\")\n",
    "print(\"keras version \"+keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genData(): \n",
    "    a1=random.randint(0,127)\n",
    "    a2=random.randint(0,127)\n",
    "    a3=a1+a2\n",
    "    tmp1=np.unpackbits(np.uint8(a1))\n",
    "    tmp2=np.unpackbits(np.uint8(a2))\n",
    "    y=np.unpackbits(np.uint8(a3))\n",
    "    tmp1=tmp1[::-1]\n",
    "    tmp2=tmp2[::-1]\n",
    "    y=y[::-1]\n",
    "    tmp1=np.reshape(tmp1, (8,1))\n",
    "    tmp2=np.reshape(tmp2, (8,1))\n",
    "    X=np.concatenate((tmp1, tmp2),1)\n",
    "    y=np.reshape(y,(8,1))\n",
    "    X=np.expand_dims(X,0)\n",
    "    y=np.expand_dims(y,0)\n",
    "    return(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0]\n",
      "  [1 1]\n",
      "  [0 1]\n",
      "  [0 0]\n",
      "  [1 0]\n",
      "  [1 1]\n",
      "  [0 1]\n",
      "  [0 0]]]\n",
      "[[[0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [1]\n",
      "  [1]\n",
      "  [0]\n",
      "  [0]\n",
      "  [1]]]\n"
     ]
    }
   ],
   "source": [
    "a,b=genData()\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Percentage: 6.20001240002\n"
     ]
    }
   ],
   "source": [
    "tmpX, tmpy=genData()\n",
    "X=tmpX\n",
    "y=tmpy\n",
    "# Note the total combination is 127 x 127\n",
    "NumSample=1000\n",
    "print(\"Sample Percentage: \"+str(NumSample/(127.*127.)*100))\n",
    "for i in range(NumSample-1):\n",
    "  tmpX, tmpy=genData()\n",
    "  X=np.insert(X, 1, tmpX, axis=0)\n",
    "  y=np.insert(y, 1, tmpy, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 8, 2)\n",
      "(1000, 8, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(670, 8, 2)\n",
      "(670, 8, 1)\n",
      "Training Sample Percentage: 4.15400830802\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"Training Sample Percentage: \"+str(X_train.shape[0]/(127.*127.)*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 569 samples, validate on 101 samples\n",
      "Epoch 1/200\n",
      "569/569 [==============================] - 9s - loss: 0.2583 - val_loss: 0.2509\n",
      "Epoch 2/200\n",
      "569/569 [==============================] - 7s - loss: 0.2492 - val_loss: 0.2487\n",
      "Epoch 3/200\n",
      "569/569 [==============================] - 7s - loss: 0.2486 - val_loss: 0.2479\n",
      "Epoch 4/200\n",
      "569/569 [==============================] - 8s - loss: 0.2482 - val_loss: 0.2476\n",
      "Epoch 5/200\n",
      "569/569 [==============================] - 8s - loss: 0.2480 - val_loss: 0.2476\n",
      "Epoch 6/200\n",
      "569/569 [==============================] - 7s - loss: 0.2479 - val_loss: 0.2471\n",
      "Epoch 7/200\n",
      "569/569 [==============================] - 9s - loss: 0.2476 - val_loss: 0.2470\n",
      "Epoch 8/200\n",
      "569/569 [==============================] - 7s - loss: 0.2477 - val_loss: 0.2470\n",
      "Epoch 9/200\n",
      "569/569 [==============================] - 7s - loss: 0.2477 - val_loss: 0.2494\n",
      "Epoch 10/200\n",
      "569/569 [==============================] - 7s - loss: 0.2474 - val_loss: 0.2478\n",
      "Epoch 11/200\n",
      "569/569 [==============================] - 6s - loss: 0.2473 - val_loss: 0.2470\n",
      "Epoch 12/200\n",
      "569/569 [==============================] - 6s - loss: 0.2474 - val_loss: 0.2467\n",
      "Epoch 13/200\n",
      "569/569 [==============================] - 6s - loss: 0.2473 - val_loss: 0.2492\n",
      "Epoch 14/200\n",
      "569/569 [==============================] - 8s - loss: 0.2474 - val_loss: 0.2469\n",
      "Epoch 15/200\n",
      "569/569 [==============================] - 8s - loss: 0.2473 - val_loss: 0.2478\n",
      "Epoch 16/200\n",
      "569/569 [==============================] - 6s - loss: 0.2471 - val_loss: 0.2475\n",
      "Epoch 17/200\n",
      "569/569 [==============================] - 8s - loss: 0.2473 - val_loss: 0.2468\n",
      "Epoch 18/200\n",
      "569/569 [==============================] - 8s - loss: 0.2471 - val_loss: 0.2477\n",
      "Epoch 19/200\n",
      "569/569 [==============================] - 7s - loss: 0.2472 - val_loss: 0.2479\n",
      "Epoch 20/200\n",
      "569/569 [==============================] - 7s - loss: 0.2471 - val_loss: 0.2465\n",
      "Epoch 21/200\n",
      "569/569 [==============================] - 7s - loss: 0.2471 - val_loss: 0.2464\n",
      "Epoch 22/200\n",
      "569/569 [==============================] - 9s - loss: 0.2471 - val_loss: 0.2466\n",
      "Epoch 23/200\n",
      "569/569 [==============================] - 7s - loss: 0.2469 - val_loss: 0.2469\n",
      "Epoch 24/200\n",
      "569/569 [==============================] - 6s - loss: 0.2470 - val_loss: 0.2463\n",
      "Epoch 25/200\n",
      "569/569 [==============================] - 8s - loss: 0.2468 - val_loss: 0.2467\n",
      "Epoch 26/200\n",
      "569/569 [==============================] - 8s - loss: 0.2469 - val_loss: 0.2467\n",
      "Epoch 27/200\n",
      "569/569 [==============================] - 7s - loss: 0.2466 - val_loss: 0.2475\n",
      "Epoch 28/200\n",
      "569/569 [==============================] - 6s - loss: 0.2468 - val_loss: 0.2463\n",
      "Epoch 29/200\n",
      "569/569 [==============================] - 7s - loss: 0.2466 - val_loss: 0.2514\n",
      "Epoch 30/200\n",
      "569/569 [==============================] - 7s - loss: 0.2465 - val_loss: 0.2474\n",
      "Epoch 31/200\n",
      "569/569 [==============================] - 6s - loss: 0.2467 - val_loss: 0.2461\n",
      "Epoch 32/200\n",
      "569/569 [==============================] - 6s - loss: 0.2465 - val_loss: 0.2470\n",
      "Epoch 33/200\n",
      "569/569 [==============================] - 6s - loss: 0.2463 - val_loss: 0.2460\n",
      "Epoch 34/200\n",
      "569/569 [==============================] - 6s - loss: 0.2464 - val_loss: 0.2460\n",
      "Epoch 35/200\n",
      "569/569 [==============================] - 10s - loss: 0.2463 - val_loss: 0.2486\n",
      "Epoch 36/200\n",
      "569/569 [==============================] - 7s - loss: 0.2461 - val_loss: 0.2458\n",
      "Epoch 37/200\n",
      "569/569 [==============================] - 6s - loss: 0.2463 - val_loss: 0.2458\n",
      "Epoch 38/200\n",
      "569/569 [==============================] - 8s - loss: 0.2461 - val_loss: 0.2457\n",
      "Epoch 39/200\n",
      "569/569 [==============================] - 6s - loss: 0.2461 - val_loss: 0.2460\n",
      "Epoch 40/200\n",
      "569/569 [==============================] - 8s - loss: 0.2461 - val_loss: 0.2456\n",
      "Epoch 41/200\n",
      "569/569 [==============================] - 6s - loss: 0.2458 - val_loss: 0.2455\n",
      "Epoch 42/200\n",
      "569/569 [==============================] - 6s - loss: 0.2457 - val_loss: 0.2467\n",
      "Epoch 43/200\n",
      "569/569 [==============================] - 7s - loss: 0.2458 - val_loss: 0.2453\n",
      "Epoch 44/200\n",
      "569/569 [==============================] - 6s - loss: 0.2455 - val_loss: 0.2454\n",
      "Epoch 45/200\n",
      "569/569 [==============================] - 8s - loss: 0.2455 - val_loss: 0.2452\n",
      "Epoch 46/200\n",
      "569/569 [==============================] - 7s - loss: 0.2455 - val_loss: 0.2459\n",
      "Epoch 47/200\n",
      "569/569 [==============================] - 8s - loss: 0.2453 - val_loss: 0.2502\n",
      "Epoch 48/200\n",
      "569/569 [==============================] - 6s - loss: 0.2455 - val_loss: 0.2451\n",
      "Epoch 49/200\n",
      "569/569 [==============================] - 7s - loss: 0.2454 - val_loss: 0.2449\n",
      "Epoch 50/200\n",
      "569/569 [==============================] - 8s - loss: 0.2452 - val_loss: 0.2464\n",
      "Epoch 51/200\n",
      "569/569 [==============================] - 8s - loss: 0.2450 - val_loss: 0.2449\n",
      "Epoch 52/200\n",
      "569/569 [==============================] - 7s - loss: 0.2448 - val_loss: 0.2450\n",
      "Epoch 53/200\n",
      "569/569 [==============================] - 7s - loss: 0.2445 - val_loss: 0.2444\n",
      "Epoch 54/200\n",
      "569/569 [==============================] - 7s - loss: 0.2447 - val_loss: 0.2444\n",
      "Epoch 55/200\n",
      "569/569 [==============================] - 8s - loss: 0.2444 - val_loss: 0.2442\n",
      "Epoch 56/200\n",
      "569/569 [==============================] - 7s - loss: 0.2442 - val_loss: 0.2445\n",
      "Epoch 57/200\n",
      "569/569 [==============================] - 7s - loss: 0.2439 - val_loss: 0.2439\n",
      "Epoch 58/200\n",
      "569/569 [==============================] - 6s - loss: 0.2436 - val_loss: 0.2471\n",
      "Epoch 59/200\n",
      "569/569 [==============================] - 8s - loss: 0.2435 - val_loss: 0.2444\n",
      "Epoch 60/200\n",
      "569/569 [==============================] - 7s - loss: 0.2435 - val_loss: 0.2433\n",
      "Epoch 61/200\n",
      "569/569 [==============================] - 6s - loss: 0.2432 - val_loss: 0.2442\n",
      "Epoch 62/200\n",
      "569/569 [==============================] - 8s - loss: 0.2430 - val_loss: 0.2436\n",
      "Epoch 63/200\n",
      "569/569 [==============================] - 8s - loss: 0.2430 - val_loss: 0.2431\n",
      "Epoch 64/200\n",
      "569/569 [==============================] - 9s - loss: 0.2427 - val_loss: 0.2425\n",
      "Epoch 65/200\n",
      "569/569 [==============================] - 6s - loss: 0.2424 - val_loss: 0.2423\n",
      "Epoch 66/200\n",
      "569/569 [==============================] - 7s - loss: 0.2421 - val_loss: 0.2421\n",
      "Epoch 67/200\n",
      "569/569 [==============================] - 6s - loss: 0.2418 - val_loss: 0.2419\n",
      "Epoch 68/200\n",
      "569/569 [==============================] - 6s - loss: 0.2416 - val_loss: 0.2424\n",
      "Epoch 69/200\n",
      "569/569 [==============================] - 6s - loss: 0.2410 - val_loss: 0.2412\n",
      "Epoch 70/200\n",
      "569/569 [==============================] - 10s - loss: 0.2410 - val_loss: 0.2430\n",
      "Epoch 71/200\n",
      "569/569 [==============================] - 7s - loss: 0.2406 - val_loss: 0.2408\n",
      "Epoch 72/200\n",
      "569/569 [==============================] - 7s - loss: 0.2403 - val_loss: 0.2403\n",
      "Epoch 73/200\n",
      "569/569 [==============================] - 8s - loss: 0.2398 - val_loss: 0.2411\n",
      "Epoch 74/200\n",
      "569/569 [==============================] - 7s - loss: 0.2396 - val_loss: 0.2397\n",
      "Epoch 75/200\n",
      "569/569 [==============================] - 7s - loss: 0.2391 - val_loss: 0.2392\n",
      "Epoch 76/200\n",
      "569/569 [==============================] - 7s - loss: 0.2386 - val_loss: 0.2394\n",
      "Epoch 77/200\n",
      "569/569 [==============================] - 7s - loss: 0.2380 - val_loss: 0.2399\n",
      "Epoch 78/200\n",
      "569/569 [==============================] - 10s - loss: 0.2375 - val_loss: 0.2382\n",
      "Epoch 79/200\n",
      "569/569 [==============================] - 7s - loss: 0.2372 - val_loss: 0.2374\n",
      "Epoch 80/200\n",
      "569/569 [==============================] - 7s - loss: 0.2365 - val_loss: 0.2376\n",
      "Epoch 81/200\n",
      "569/569 [==============================] - 9s - loss: 0.2359 - val_loss: 0.2366\n",
      "Epoch 82/200\n",
      "569/569 [==============================] - 6s - loss: 0.2352 - val_loss: 0.2406\n",
      "Epoch 83/200\n",
      "569/569 [==============================] - 6s - loss: 0.2348 - val_loss: 0.2385\n",
      "Epoch 84/200\n",
      "569/569 [==============================] - 7s - loss: 0.2342 - val_loss: 0.2343\n",
      "Epoch 85/200\n",
      "569/569 [==============================] - 7s - loss: 0.2334 - val_loss: 0.2348\n",
      "Epoch 86/200\n",
      "569/569 [==============================] - 7s - loss: 0.2327 - val_loss: 0.2349\n",
      "Epoch 87/200\n",
      "569/569 [==============================] - 7s - loss: 0.2319 - val_loss: 0.2328\n",
      "Epoch 88/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569/569 [==============================] - 7s - loss: 0.2312 - val_loss: 0.2312\n",
      "Epoch 89/200\n",
      "569/569 [==============================] - 7s - loss: 0.2300 - val_loss: 0.2302\n",
      "Epoch 90/200\n",
      "569/569 [==============================] - 7s - loss: 0.2289 - val_loss: 0.2292\n",
      "Epoch 91/200\n",
      "569/569 [==============================] - 7s - loss: 0.2281 - val_loss: 0.2286\n",
      "Epoch 92/200\n",
      "569/569 [==============================] - 8s - loss: 0.2271 - val_loss: 0.2271\n",
      "Epoch 93/200\n",
      "569/569 [==============================] - 8s - loss: 0.2258 - val_loss: 0.2258\n",
      "Epoch 94/200\n",
      "569/569 [==============================] - 7s - loss: 0.2245 - val_loss: 0.2267\n",
      "Epoch 95/200\n",
      "569/569 [==============================] - 6s - loss: 0.2234 - val_loss: 0.2231\n",
      "Epoch 96/200\n",
      "569/569 [==============================] - 8s - loss: 0.2220 - val_loss: 0.2221\n",
      "Epoch 97/200\n",
      "569/569 [==============================] - 7s - loss: 0.2205 - val_loss: 0.2211\n",
      "Epoch 98/200\n",
      "569/569 [==============================] - 8s - loss: 0.2186 - val_loss: 0.2212\n",
      "Epoch 99/200\n",
      "569/569 [==============================] - 10s - loss: 0.2175 - val_loss: 0.2177\n",
      "Epoch 100/200\n",
      "569/569 [==============================] - 7s - loss: 0.2158 - val_loss: 0.2149\n",
      "Epoch 101/200\n",
      "569/569 [==============================] - 8s - loss: 0.2137 - val_loss: 0.2135\n",
      "Epoch 102/200\n",
      "569/569 [==============================] - 8s - loss: 0.2118 - val_loss: 0.2109\n",
      "Epoch 103/200\n",
      "569/569 [==============================] - 9s - loss: 0.2099 - val_loss: 0.2111\n",
      "Epoch 104/200\n",
      "569/569 [==============================] - 6s - loss: 0.2080 - val_loss: 0.2068\n",
      "Epoch 105/200\n",
      "569/569 [==============================] - 7s - loss: 0.2057 - val_loss: 0.2060\n",
      "Epoch 106/200\n",
      "569/569 [==============================] - 7s - loss: 0.2036 - val_loss: 0.2039\n",
      "Epoch 107/200\n",
      "569/569 [==============================] - 7s - loss: 0.2016 - val_loss: 0.2013\n",
      "Epoch 108/200\n",
      "569/569 [==============================] - 6s - loss: 0.1994 - val_loss: 0.1974\n",
      "Epoch 109/200\n",
      "569/569 [==============================] - 7s - loss: 0.1971 - val_loss: 0.1949\n",
      "Epoch 110/200\n",
      "569/569 [==============================] - 6s - loss: 0.1944 - val_loss: 0.1924\n",
      "Epoch 111/200\n",
      "569/569 [==============================] - 7s - loss: 0.1923 - val_loss: 0.1893\n",
      "Epoch 112/200\n",
      "569/569 [==============================] - 7s - loss: 0.1896 - val_loss: 0.1885\n",
      "Epoch 113/200\n",
      "569/569 [==============================] - 7s - loss: 0.1873 - val_loss: 0.1842\n",
      "Epoch 114/200\n",
      "569/569 [==============================] - 8s - loss: 0.1846 - val_loss: 0.1813\n",
      "Epoch 115/200\n",
      "569/569 [==============================] - 7s - loss: 0.1824 - val_loss: 0.1790\n",
      "Epoch 116/200\n",
      "569/569 [==============================] - 8s - loss: 0.1800 - val_loss: 0.1764\n",
      "Epoch 117/200\n",
      "569/569 [==============================] - 7s - loss: 0.1776 - val_loss: 0.1734\n",
      "Epoch 118/200\n",
      "569/569 [==============================] - 7s - loss: 0.1754 - val_loss: 0.1712\n",
      "Epoch 119/200\n",
      "569/569 [==============================] - 6s - loss: 0.1726 - val_loss: 0.1688\n",
      "Epoch 120/200\n",
      "569/569 [==============================] - 6s - loss: 0.1702 - val_loss: 0.1653\n",
      "Epoch 121/200\n",
      "569/569 [==============================] - 8s - loss: 0.1669 - val_loss: 0.1648\n",
      "Epoch 122/200\n",
      "569/569 [==============================] - 7s - loss: 0.1642 - val_loss: 0.1637\n",
      "Epoch 123/200\n",
      "569/569 [==============================] - 6s - loss: 0.1610 - val_loss: 0.1561\n",
      "Epoch 124/200\n",
      "569/569 [==============================] - 8s - loss: 0.1575 - val_loss: 0.1522\n",
      "Epoch 125/200\n",
      "569/569 [==============================] - 7s - loss: 0.1536 - val_loss: 0.1508\n",
      "Epoch 126/200\n",
      "569/569 [==============================] - 7s - loss: 0.1497 - val_loss: 0.1455\n",
      "Epoch 127/200\n",
      "569/569 [==============================] - 6s - loss: 0.1457 - val_loss: 0.1403\n",
      "Epoch 128/200\n",
      "569/569 [==============================] - 6s - loss: 0.1409 - val_loss: 0.1358\n",
      "Epoch 129/200\n",
      "569/569 [==============================] - 6s - loss: 0.1359 - val_loss: 0.1307\n",
      "Epoch 130/200\n",
      "569/569 [==============================] - 6s - loss: 0.1306 - val_loss: 0.1253\n",
      "Epoch 131/200\n",
      "569/569 [==============================] - 8s - loss: 0.1244 - val_loss: 0.1202\n",
      "Epoch 132/200\n",
      "569/569 [==============================] - 7s - loss: 0.1186 - val_loss: 0.1149\n",
      "Epoch 133/200\n",
      "569/569 [==============================] - 8s - loss: 0.1120 - val_loss: 0.1096\n",
      "Epoch 134/200\n",
      "569/569 [==============================] - 7s - loss: 0.1060 - val_loss: 0.1023\n",
      "Epoch 135/200\n",
      "569/569 [==============================] - 6s - loss: 0.0999 - val_loss: 0.0978\n",
      "Epoch 136/200\n",
      "569/569 [==============================] - 6s - loss: 0.0940 - val_loss: 0.0912\n",
      "Epoch 137/200\n",
      "569/569 [==============================] - 6s - loss: 0.0878 - val_loss: 0.0854\n",
      "Epoch 138/200\n",
      "569/569 [==============================] - 8s - loss: 0.0819 - val_loss: 0.0802\n",
      "Epoch 139/200\n",
      "569/569 [==============================] - 8s - loss: 0.0756 - val_loss: 0.0733\n",
      "Epoch 140/200\n",
      "569/569 [==============================] - 8s - loss: 0.0691 - val_loss: 0.0678\n",
      "Epoch 141/200\n",
      "569/569 [==============================] - 8s - loss: 0.0622 - val_loss: 0.0602\n",
      "Epoch 142/200\n",
      "569/569 [==============================] - 9s - loss: 0.0549 - val_loss: 0.0527\n",
      "Epoch 143/200\n",
      "569/569 [==============================] - 6s - loss: 0.0476 - val_loss: 0.0445\n",
      "Epoch 144/200\n",
      "569/569 [==============================] - 8s - loss: 0.0400 - val_loss: 0.0377\n",
      "Epoch 145/200\n",
      "569/569 [==============================] - 8s - loss: 0.0323 - val_loss: 0.0294\n",
      "Epoch 146/200\n",
      "569/569 [==============================] - 8s - loss: 0.0262 - val_loss: 0.0241\n",
      "Epoch 147/200\n",
      "569/569 [==============================] - 6s - loss: 0.0207 - val_loss: 0.0182\n",
      "Epoch 148/200\n",
      "569/569 [==============================] - 6s - loss: 0.0162 - val_loss: 0.0150\n",
      "Epoch 149/200\n",
      "569/569 [==============================] - 6s - loss: 0.0128 - val_loss: 0.0126\n",
      "Epoch 150/200\n",
      "569/569 [==============================] - 7s - loss: 0.0110 - val_loss: 0.0098\n",
      "Epoch 151/200\n",
      "569/569 [==============================] - 9s - loss: 0.0093 - val_loss: 0.0083\n",
      "Epoch 152/200\n",
      "569/569 [==============================] - 7s - loss: 0.0083 - val_loss: 0.0074\n",
      "Epoch 153/200\n",
      "569/569 [==============================] - 6s - loss: 0.0075 - val_loss: 0.0070\n",
      "Epoch 154/200\n",
      "569/569 [==============================] - 6s - loss: 0.0069 - val_loss: 0.0064\n",
      "Epoch 155/200\n",
      "569/569 [==============================] - 7s - loss: 0.0064 - val_loss: 0.0059\n",
      "Epoch 156/200\n",
      "569/569 [==============================] - 8s - loss: 0.0059 - val_loss: 0.0055\n",
      "Epoch 157/200\n",
      "569/569 [==============================] - 7s - loss: 0.0055 - val_loss: 0.0052\n",
      "Epoch 158/200\n",
      "569/569 [==============================] - 7s - loss: 0.0052 - val_loss: 0.0049\n",
      "Epoch 159/200\n",
      "569/569 [==============================] - 7s - loss: 0.0050 - val_loss: 0.0046\n",
      "Epoch 160/200\n",
      "569/569 [==============================] - 10s - loss: 0.0047 - val_loss: 0.0044\n",
      "Epoch 161/200\n",
      "569/569 [==============================] - 7s - loss: 0.0045 - val_loss: 0.0043\n",
      "Epoch 162/200\n",
      "569/569 [==============================] - 7s - loss: 0.0043 - val_loss: 0.0041\n",
      "Epoch 163/200\n",
      "569/569 [==============================] - 8s - loss: 0.0041 - val_loss: 0.0039\n",
      "Epoch 164/200\n",
      "569/569 [==============================] - 7s - loss: 0.0039 - val_loss: 0.0037\n",
      "Epoch 165/200\n",
      "569/569 [==============================] - 8s - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 166/200\n",
      "569/569 [==============================] - 7s - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 167/200\n",
      "569/569 [==============================] - 8s - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 168/200\n",
      "569/569 [==============================] - 7s - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 169/200\n",
      "569/569 [==============================] - 8s - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 170/200\n",
      "569/569 [==============================] - 7s - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 171/200\n",
      "569/569 [==============================] - 8s - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 172/200\n",
      "569/569 [==============================] - 7s - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 173/200\n",
      "569/569 [==============================] - 8s - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 174/200\n",
      "569/569 [==============================] - 7s - loss: 0.0028 - val_loss: 0.0026\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569/569 [==============================] - 7s - loss: 0.0027 - val_loss: 0.0025\n",
      "Epoch 176/200\n",
      "569/569 [==============================] - 7s - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 177/200\n",
      "569/569 [==============================] - 7s - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 178/200\n",
      "569/569 [==============================] - 7s - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 179/200\n",
      "569/569 [==============================] - 7s - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 180/200\n",
      "569/569 [==============================] - 6s - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 181/200\n",
      "569/569 [==============================] - 7s - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 182/200\n",
      "569/569 [==============================] - 6s - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 183/200\n",
      "569/569 [==============================] - 6s - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 184/200\n",
      "569/569 [==============================] - 6s - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 185/200\n",
      "569/569 [==============================] - 7s - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 186/200\n",
      "569/569 [==============================] - 8s - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 187/200\n",
      "569/569 [==============================] - 8s - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 188/200\n",
      "569/569 [==============================] - 7s - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 189/200\n",
      "569/569 [==============================] - 7s - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 190/200\n",
      "569/569 [==============================] - 7s - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 191/200\n",
      "569/569 [==============================] - 7s - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 192/200\n",
      "569/569 [==============================] - 8s - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 193/200\n",
      "569/569 [==============================] - 7s - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 194/200\n",
      "569/569 [==============================] - 6s - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 195/200\n",
      "569/569 [==============================] - 8s - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 196/200\n",
      "569/569 [==============================] - 8s - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 197/200\n",
      "569/569 [==============================] - 7s - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 198/200\n",
      "569/569 [==============================] - 7s - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 199/200\n",
      "569/569 [==============================] - 6s - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 200/200\n",
      "569/569 [==============================] - 8s - loss: 0.0015 - val_loss: 0.0015\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# Build the model.\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "isCheckpoint=False\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(LSTM(256, input_dim=max_features, return_sequences=True))\n",
    "feature_dim=2\n",
    "model.add(LSTM(10, input_shape=(None, feature_dim), return_sequences=True))\n",
    "#model.add(LSTM(256, return_sequences=True))\n",
    "model.add(LSTM(10, return_sequences=True))\n",
    "model.add(Dense(1))  #is it dense? or add(LSTM(max_features))\n",
    "#model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "\n",
    "# It is a best practice to use checkpoint \n",
    "callbacks_list=None\n",
    "if isCheckpoint==True:\n",
    "  filepath=\"weight/RNN_BinaryAddition-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "  checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "  callbacks_list = [checkpoint]\n",
    "\n",
    "history=model.fit(X_train, y_train, validation_split=0.15 ,batch_size=1, epochs=200,callbacks=callbacks_list)\n",
    "\n",
    "print(\"finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndxFOlFEVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3926f90450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321/330 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=1)\n",
    "\n",
    "# predict(self, x, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.00135907424897\n"
     ]
    }
   ],
   "source": [
    "print(\"score: \"+str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(330, 8, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_y=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(330, 8, 1)\n"
     ]
    }
   ],
   "source": [
    "print(predict_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test[0,...])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.98763132]\n",
      " [-0.00283545]\n",
      " [-0.00467482]\n",
      " [-0.01041976]\n",
      " [ 1.01300597]\n",
      " [-0.00527069]\n",
      " [-0.00333282]\n",
      " [ 1.00169396]]\n"
     ]
    }
   ],
   "source": [
    "print(predict_y[0,...])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#10001001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
